---
layout: post
title: "Gradient boosting and AdaBoost"
date: 2017-11-06
categories: MachineLearning
---



## Boosting and Bagging

When we build a predictive model using machine learning, one can reduce the variance (over-fitting) of the model by using a collection of weak predictors (learners) instead of one strong learner. This ensemble method is further classified into *boosting* and *bagging*.
<!--more-->

In bagging, many weak learners are created in parallel, and the prediction is made by combining these weak learners using a averaging method such as majority vote. An example of bagging is Random Forest regressor.

In boosting, weak learners are created in a sequence such that the newly created predictors learn from previous mistakes. Gradient boosting and AdaBoost are, as the names suggest, examples of the boosting techniques.


## AdaBoost

Adaptive Boosting or AdaBoost was the first boosting algorithm developed. In AdaBoost, weak learners are created by focusing on the mistakes made by previous learners. The simplified procedure follows.

1. A weak learner is created. Predictions are made by the weak learner, $$F_{i}(x_{i}) = y_{i}^{p}$$.
2. The weak learner is added to the strong leaner according to its performance, $$F_{sum} = F_{sum} + F_{i}$$.
2. The samples that are correctly predicted by the previous weak learners are weighted less, and the samples with incorrect predictions are weighted more, $$x_{i} \rightarrow x_{i+1}$$ where $$x_{i}$$ represent ith reweighted sample.
3. The next weak learner is created in a sequence using the weighted samples, $$F_{i+1}(x_{i+1}) = y_{i+1}^{p}$$.
4. Repeat 1-3 until the specified number of weak learners is created (or other condition is satisfied).

After creating and combining weak learners, predictions are made by vote of the weak learners.


## Gradient Boosting

In Gradient Boosting, the concept of gradient descent is used to minimize the loss function. For example, the mean squared error loss function is defined as,

$$MSE = \Sigma(y_{i} - y_{i}^{p})^{2},$$

where $$y_{i}$$ is ith target value and $$y_{i}^{p}$$ is the prediction for ith target. In regression or classification problem, the goal is to build a model that minimizes the loss function. This can be achieved by iterating $$y_{i}^{p}$$ using gradient descent. i.e.

$$y_{i}^{p} = y_{i}^{p} + \alpha \cdot \frac{\delta}{\delta y_{i}^{p}}\cdot(MSE)= y_{i}^{p} + \alpha \cdot \frac{\delta}{\delta y_{i}^{p}} \cdot (\Sigma(y_{i} - y_{i}^{p})^{2})$$


So Gradient Boosting proceeds as follows:

1. Gradient Boosting generates the weak learner and fit data, $$F_{sum} = F_{1}(x) = y_{1}^{p}$$.
2. The residuals are calculated from the prediction, $$h_{1}(x) = y - y_{1}^{p}$$.
3. The next weak learner is trained on the residual, $$F_{2}(h_{1})$$.
4. The weak learner is added to the strong leaner , $$F_{sum} = F_{sum} + F_{2}$$.
5. Repeat step 2-4


In both algorithms, a set of weak learners are combined into a single strong leaner. Because we are adding many weak learners, which can be over-fit as individual predictor, we can reduce high variance of the model. One of the important differences between Gradient Boosting and AdaBoost is that in AdaBoost, we change the training samples by reweighting whereas the training examples remain un-changed in Gradient Boosting. Although both algorithms are useful in different cases, often Gradient Boosting is favored for better performance in more situations.

